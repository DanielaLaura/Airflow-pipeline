# Written By: Laura M.
# Last Update: 29th January 2020
# Caveat: This Dag will not run because of missing scripts.
# The purpose of this is to give you a sample of a real world example DAG!
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# Load The Dependencies
# --------------------------------------------------------------------------------

import airflow
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.hive_operator import HiveOperator
from datetime import date, timedelta
from airflow.operators.kubernetes_operator import KubernetesPodOperator
#import application
#import features_and_labels
#from prediction_engineering import label_customer
#import main
# --------------------------------------------------------------------------------
# Create a few placeholder scripts. In practice these would be different python
# script files, which are imported in this section with absolute or relative imports
# --------------------------------------------------------------------------------


def fetch_data_from_api():
    parameters = {
        "lat": 40.71,
        "lon": -74
    }
    response = requests.get("http://api.open-notify.org/playerinfo.json", params=parameters)
    return response


def clean_data():
    return None


def data_labeling():
    return None


def features_and_labels():
    return None


def fit_model():
    return None


def model_degradation():
    return None


def transfertodb():
    return None


# --------------------------------------------------------------------------------
# set default arguments
# --------------------------------------------------------------------------------

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(2),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}

dag = DAG(
    'ml_example_dag', default_args=default_args,
    schedule_interval="@daily")

# --------------------------------------------------------------------------------
# This task should call an API and retrieve data from yesterday from
#  four game users (player_A,..,player_D) There should be n csv/json
# output files generated by this task and naming convention
#Clean the n files. In this step you can get rid of or cherry pick columns, missing values, outliers
# and different parts of the text
# --------------------------------------------------------------------------------

fetch_data = PythonOperator(
    task_id='fetch_data',
    python_callable=get_data.import_clean_data,
    dag=dag)


# --------------------------------------------------------------------------------
# In this section you can use a script to label the historical data.
# --------------------------------------------------------------------------------

data_labeling = PythonOperator(
    task_id='data_labeling',
    python_callable= prediction_enginering_all.make_label_times,
    dag=dag)

data_labeling.set_upstream(fetch_data)

# --------------------------------------------------------------------------------
# In this section you can use a script to perform feature engineering.

# --------------------------------------------------------------------------------
features_and_labels = PythonOperator(
    task_id='features_and_labels',
    python_callable=features_and_labels.features_and_labels,
    dag=dag)

features_and_labels.set_upstream(data_labeling)
# --------------------------------------------------------------------------------
# Here you can call the  model trained (Doker image to keep dependencies and further
# use the Kubernetes operator to scale) on historical data and fit it on the new data
# --------------------------------------------------------------------------------
train_model = KubernetesPodOperator(
    task_id="training",
    name="airflowjob-{}-model-training-{}".format(env, uuid),
    namespace="default",
    image="docker/image/url:latest",
    cmds=[
        "python",
        "app/algorithm.py",
        "--env",
        env,
        "--uuid",
        uuid,
    ],
    in_cluster=IN_CLUSTER,
    cluster_context=CLUSTER_CONTEXT,
    get_logs=True,
    startup_timeout_seconds=60 * 5,
    is_delete_operator_pod=True,
    resources=Resources(request_cpu="100m", request_memory="1Gi"),
    image_pull_policy="Always",
    node_selectors={
        "cloud.google.com/gke-nodepool": "n1-standard-4-pool"
    },
    tolerations=[
        {
            "key": "node-pool",
            "operator": "Equal",
            "value": "n1-standard-4-pool",
            "effect": "NoExecute",
        }
    ],
    retries=1,
    retry_delay=timedelta(minutes=5),
    xcom_push=True,
    dag=dag,
)



#same task but with a callable python file
#fit_model = PythonOperator(
 #   task_id=' fit_model',
    #python_callable=fit_model,
    #dag=dag)

#fit_model.set_upstream(features_and_labels)

# --------------------------------------------------------------------------------
# As a model starts to degrade over time due to new data, there must be an automated
# trigger to set a specific threshold regarding its performance
# --------------------------------------------------------------------------------

model_degradation = PythonOperator(
    task_id='features_and_labels',
    python_callable=model_degradation,
    dag=dag)
model_degradation.set_upstream(fit_model)

# --------------------------------------------------------------------------------
# Although this is the last task, we need to declare it before the next tasks as we
# will use set_downstream This task will extract summary from Hive data and store
# it to postgres
# --------------------------------------------------------------------------------

hive_to_postgres = PythonOperator(
    task_id='hive_to_mysql',
    python_callable=transfertodb,
    dag=dag)

# --------------------------------------------------------------------------------
# The following tasks are generated using for loop. The first task puts the n
# csv files to HDFS. The second task loads these files from HDFS to respective Hive
# tables.
# --------------------------------------------------------------------------------

from_channels = ['fromPlayer_A', 'fromPlayer_B', 'fromPlayer_C', 'fromPlayer_D']

yesterday = date.today() - timedelta(days=1)
dt = yesterday.strftime("%Y-%m-%d")
# define where you want to store the tweets csv file in your local directory
local_dir = "/tmp/"
# define the location where you want to store in HDFS
hdfs_dir = " /tmp/"

for channel in to_channels:

    file_name = "from_" + channel + "_" + yesterday.strftime("%Y-%m-%d") + ".csv"

    load_to_hdfs = BashOperator(
        task_id="put_" + channel + "_to_hdfs",
        bash_command="HADOOP_USER_NAME=hdfs hadoop fs -put -f " +
                     local_dir + file_name +
                     hdfs_dir + channel + "/",
        dag=dag)

    load_to_hdfs.set_upstream(clean_data)

    load_to_hive = HiveOperator(
        task_id="load_" + channel + "_to_hive",
        hql="LOAD DATA INPATH '" +
            hdfs_dir + channel + "/" + file_name + "' "
            "INTO TABLE " + channel + " "
            "PARTITION(dt='" + dt + "')",
        dag=dag)
    load_to_hive.set_upstream(load_to_hdfs)
    load_to_hive.set_downstream(hive_to_posgres)

